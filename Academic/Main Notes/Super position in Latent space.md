2025-01-21 - 12:46

Status: Very ongoing

Tags: [[Low manifold hypothesis]] [[Mechanistic Interpretability]]

## **Untitled**

This idea is in line with hyperdimensional computing. The idea is the a single neuron not responsible for a particular representation but rather that a constellation of activations in a layer is responsible for a particular representation. This is how so many representations can be represented in far fewer spacial dimensions but also why discrete decomposition and compositionally might be so hard. Representations of distinct categories are not limited to distinct perpendicular planes. There is a paper that mentions this needs to be made a constraint but I can't remember which one - prolly this https://transformer-circuits.pub/2023/monosemantic-features


#### **References**
https://www.youtube.com/watch?v=9-Jl0dxWQs8
https://transformer-circuits.pub/2022/toy_model/index.html
https://transformer-circuits.pub/2023/monosemantic-features